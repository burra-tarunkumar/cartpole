{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504c9ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "train_env=gym.make('CartPole-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34121d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x2014db821d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=DQN(policy='MlpPolicy',\n",
    "          env=train_env,\n",
    "          learning_rate=1e-3,\n",
    "          gamma=0.99,\n",
    "          buffer_size=50000,\n",
    "          batch_size=32,\n",
    "          learning_starts=1000)\n",
    "model.learn(total_timesteps=100_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ad08de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 120.0\n"
     ]
    }
   ],
   "source": [
    "obs,_=train_env.reset()\n",
    "done=False\n",
    "total_reward=0\n",
    "while not done:\n",
    "    action,_=model.predict(obs,deterministic=True)\n",
    "    obs,reward,terminated,truncated,_=train_env.step(action)\n",
    "    done= terminated or truncated\n",
    "    total_reward+=reward\n",
    "train_env.close()\n",
    "print(f'Total reward: {total_reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b045026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,env,episodes=10):\n",
    "    rewards=[]\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        obs,_=env.reset()\n",
    "        done=False\n",
    "        total_reward=0\n",
    "        while not done:\n",
    "            action,_=model.predict(obs)\n",
    "            obs,reward,terminated,truncated,_=env.step(action)\n",
    "            done=terminated or truncated\n",
    "            total_reward+=reward\n",
    "        rewards.append(total_reward)\n",
    "    return sum(rewards)/len(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b2dea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg reward:117.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"avg reward:{evaluate_model(model,train_env)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b045625",
   "metadata": {},
   "source": [
    "###SAVE MODEL ONLY ONCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1120c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since average reward is greater than 200 , we can save the model\n",
    "model.save('dqn_cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874fc4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "model=DQN.load('../models/dqn_cartpole')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dff2366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 115.0\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('CartPole-v1',render_mode='human')\n",
    "obs,_=env.reset()\n",
    "done=False\n",
    "total_reward=0\n",
    "while not done:\n",
    "    action,_=model.predict(obs,deterministic=True)\n",
    "    obs,reward,terminated,truncated,_=env.step(action)\n",
    "    done=terminated or truncated\n",
    "    total_reward+=reward\n",
    "print(f'Total reward: {total_reward}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdf7298c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The DQN model  considers left as best and keeps moving left , thus failing to balance the pole for long.\\n Hence we try PPO algorithm which is on-policy and better suited for this task. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" The DQN model  considers left as best and keeps moving left , thus failing to balance the pole for long.\n",
    " Hence we try PPO algorithm which is on-policy and better suited for this task. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65eb80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
